{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwXubyqEbGos"
   },
   "source": [
    "# Social LSTM\n",
    "Adopted from [original social LSTM repo](https://github.com/williamleif/social-lstm). Modifications made to work with Colab, and to upgrade from Python 2 (original) to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GtQrNoefGYU"
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUqtFZUo7ndh"
   },
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8es6GkaleB7"
   },
   "outputs": [],
   "source": [
    "!wget http://snap.stanford.edu/conflict/conflict_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWBh9jbQ7-PS"
   },
   "outputs": [],
   "source": [
    "!unzip conflict_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcwwZVlJbnmm"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwKmsgrKaoEU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHdZypn_Z5So"
   },
   "outputs": [],
   "source": [
    "DATA_HOME = 'prediction'\n",
    "\n",
    "constants = {\n",
    "  'CUDA':True,\n",
    "  # 'CUDA':False,\n",
    "\n",
    "  # root directory that contains the training/testing data\n",
    "  'LOG_DIR':\".\",\n",
    "\n",
    "  #whether to show results on the test set\n",
    "  'PRINT_TEST':False,\n",
    "\n",
    "  # CONSTANTS YOU MAY WANT TO MODIFY (BUT DON\"T NEED TO)\n",
    "  'TRAIN_DATA':DATA_HOME+\"/preprocessed_train_data.pkl\",\n",
    "  'VAL_DATA':DATA_HOME+\"/preprocessed_val_data.pkl\",\n",
    "  'TEST_DATA':DATA_HOME+\"/preprocessed_test_data.pkl\",\n",
    "  'BATCH_SIZE':512,\n",
    "\n",
    "  # NOTE: THESE PREPROCESSED FILES HAVE A FIXED BATCH SIZE\n",
    "  'WORD_EMBEDS':DATA_HOME+\"/embeddings/glove_word_embeds.txt\",\n",
    "  'USER_EMBEDS':DATA_HOME+\"/embeddings/user_vecs.npy\",\n",
    "  'USER_IDS':DATA_HOME+\"/embeddings/user_vecs.vocab\",\n",
    "\n",
    "  'SUBREDDIT_EMBEDS':DATA_HOME+\"/embeddings/sub_vecs.npy\",\n",
    "  'SUBREDDIT_IDS':DATA_HOME+\"/embeddings/sub_vecs.vocab\",\n",
    "\n",
    "  'POST_INFO':DATA_HOME+\"/detailed_data/post_crosslink_info.tsv\",\n",
    "  'LABEL_INFO':DATA_HOME+\"/detailed_data/label_info.tsv\",\n",
    "  'PREPROCESSED_DATA':DATA_HOME+\"/detailed_data/tokenized_posts.tsv\",\n",
    "\n",
    "  'VOCAB_SIZE': 174558,\n",
    "  'NUM_USERS': 118381,\n",
    "  'NUM_SUBREDDITS': 51278,\n",
    "  'WORD_EMBED_DIM': 300,\n",
    "  'METAFEAT_LEN': 263,\n",
    "  'NUM_CLASSES': 1,\n",
    "  'MAX_LEN':50,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFD_z_48b7qG"
   },
   "source": [
    "### `Embeddings` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5vFbwyvdB7S"
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def ugly_normalize(vecs):\n",
    "   normalizers = np.sqrt((vecs * vecs).sum(axis=1))\n",
    "   normalizers[normalizers==0]=1\n",
    "   return (vecs.T / normalizers).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZucFYfIb9SC"
   },
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "   def __init__(self, vecsfile, vocabfile=None, normalize=True):\n",
    "      if vocabfile is None: vocabfile = vecsfile.replace(\"npy\",\"vocab\")\n",
    "      self._vecs = np.load(vecsfile)\n",
    "      # self._vocab = file(vocabfile).read().split()  # `file` removed in Python 3\n",
    "      with open(vocabfile, 'r') as f:\n",
    "        self._vocab = f.read().split()\n",
    "      if normalize:\n",
    "         self._vecs = ugly_normalize(self._vecs)\n",
    "      self._w2v = {w:i for i,w in enumerate(self._vocab)}\n",
    "\n",
    "   @classmethod\n",
    "   def load(cls, vecsfile, vocabfile=None):\n",
    "      return Embeddings(vecsfile, vocabfile)\n",
    "\n",
    "   def word2vec(self, w):\n",
    "      return self._vecs[self._w2v[w]]\n",
    "\n",
    "   def similar_to_vec(self, v, N=10):\n",
    "      sims = self._vecs.dot(v)\n",
    "      sims = heapq.nlargest(N, zip(sims,self._vocab,self._vecs))\n",
    "      return sims\n",
    "\n",
    "   def most_similar(self, word, N=10):\n",
    "      w = self._vocab.index(word)\n",
    "      sims = self._vecs.dot(self._vecs[w])\n",
    "      sims = heapq.nlargest(N, zip(sims,self._vocab))\n",
    "      return sims\n",
    "\n",
    "   def analogy(self, pos1, neg1, pos2,N=10,mult=True):\n",
    "      wvecs, vocab = self._vecs, self._vocab\n",
    "      p1 = vocab.index(pos1)\n",
    "      p2 = vocab.index(pos2)\n",
    "      n1 = vocab.index(neg1)\n",
    "      if mult:\n",
    "         p1,p2,n1 = [(1+wvecs.dot(wvecs[i]))/2 for i in (p1,p2,n1)]\n",
    "         if N == 1:\n",
    "            return max(((v,w) for v,w in zip((p1 * p2 / n1),vocab) if w not in [pos1,pos2,neg1]))\n",
    "         return heapq.nlargest(N,((v,w) for v,w in zip((p1 * p2 / n1),vocab) if w not in [pos1,pos2,neg1]))\n",
    "      else:\n",
    "         p1,p2,n1 = [(wvecs.dot(wvecs[i])) for i in (p1,p2,n1)]\n",
    "         if N == 1:\n",
    "            return max(((v,w) for v,w in zip((p1 + p2 - n1),vocab) if w not in [pos1,pos2,neg1]))\n",
    "         return heapq.nlargest(N,((v,w) for v,w in zip((p1 + p2 - n1),vocab) if w not in [pos1,pos2,neg1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKbYNl2xbq2H"
   },
   "source": [
    "### `SocialLSTM` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKGPSIt8bjKm"
   },
   "outputs": [],
   "source": [
    "class SocialLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for predicting conflict between Reddit communities.\n",
    "    Can incorporate social embeddings of users and communities/subreddits.\n",
    "    \"\"\"\n",
    "\n",
    "    def _load_glove_embeddings(self):\n",
    "        print('Loading word embeddings...')\n",
    "        with open(constants['WORD_EMBEDS']) as fp:\n",
    "            embeddings = np.empty((constants['VOCAB_SIZE'], constants['WORD_EMBED_DIM']), dtype=np.float32)\n",
    "            for i, line in enumerate(fp):\n",
    "                # embeddings[i,:] = map(float, line.split()[1:])\n",
    "                embeddings[i,:] = [float(item) for item in line.split()[1:]]\n",
    "        return embeddings\n",
    "\n",
    "    def _load_user_embeddings(self):\n",
    "        print('Loading user embeddings...')\n",
    "        embeds = Embeddings(constants['USER_EMBEDS'])\n",
    "        return embeds._vecs\n",
    "\n",
    "    def _load_subreddit_embeddings(self):\n",
    "        print('Loading subreddit embeddings...')\n",
    "        embeds = Embeddings(constants['SUBREDDIT_EMBEDS'])\n",
    "        return embeds._vecs\n",
    "\n",
    "    def __init__(self, hidden_dim, batch_size=constants['BATCH_SIZE'], prepend_social=True, include_meta=False,\n",
    "            dropout=None, final_dense=True, include_embeds=False):\n",
    "        \"\"\"\n",
    "        hidden_dim - size of internal LSTM layers\n",
    "        batch_size - size of minibatches during training\n",
    "        preprend_social - if True then user/subreddit embeds are prepended.\n",
    "                          if False then user/subreddit embeds are appended.\n",
    "                          if None then user/subreddit embeds are not fed to the LSTM.\n",
    "        include_meta - if True then metadata/linguistic/hand-engineered features are included\n",
    "        dropout - how much dropout in the LSTM layer connections; if None then single-layer LSTM is used.\n",
    "        final_dense - whether to include an extra dense Linear+ReLU layer before the softmax (same dimension as LSTM)\n",
    "        include_embeds - whether to include the user/subreddit layers in the final (i.e, post-lstm) layer(s)\n",
    "        \"\"\"\n",
    "        super(SocialLSTM, self).__init__()\n",
    "        glove_embeds = self._load_glove_embeddings()\n",
    "        self.glove_embeds= torch.FloatTensor(glove_embeds)\n",
    "        self.pad_embed = torch.zeros(1, constants['WORD_EMBED_DIM'])\n",
    "        self.unk_embed = torch.FloatTensor(1,constants['WORD_EMBED_DIM'])\n",
    "        self.unk_embed.normal_(std=1./np.sqrt(constants['WORD_EMBED_DIM']))\n",
    "        self.word_embeds = nn.Parameter(torch.cat([self.glove_embeds, self.pad_embed, self.unk_embed], dim=0), requires_grad=False)\n",
    "        self.embed_module = torch.nn.Embedding(constants['VOCAB_SIZE']+2, constants['WORD_EMBED_DIM'])\n",
    "        self.embed_module.weight = self.word_embeds\n",
    "\n",
    "        user_embeds = self._load_user_embeddings()\n",
    "        self.user_embeds = torch.nn.Embedding(constants['NUM_USERS']+1, constants['WORD_EMBED_DIM'])\n",
    "        self.user_embeds.weight  = nn.Parameter(torch.cat([torch.FloatTensor(user_embeds),\n",
    "            self.pad_embed]), requires_grad=False)\n",
    "\n",
    "        subreddit_embeds = self._load_subreddit_embeddings()\n",
    "        self.subreddit_embeds = torch.nn.Embedding(constants['NUM_SUBREDDITS']+1, constants['WORD_EMBED_DIM'])\n",
    "        self.subreddit_embeds.weight  = nn.Parameter(torch.cat([torch.FloatTensor(subreddit_embeds),\n",
    "            self.pad_embed]), requires_grad=False)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prepend_social = prepend_social\n",
    "\n",
    "        init_hidden_data = torch.zeros(1 if dropout is None else 2, batch_size, self.hidden_dim)\n",
    "        #init_hidden_data.normal_(std=1./np.sqrt(self.hidden_dim))\n",
    "        if constants['CUDA']:\n",
    "            init_hidden_data = init_hidden_data.cuda()\n",
    "        self.init_hidden = (Variable(init_hidden_data, requires_grad=False),\n",
    "            Variable(init_hidden_data, requires_grad=False))\n",
    "\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=constants['WORD_EMBED_DIM'], hidden_size=hidden_dim,\n",
    "                num_layers=1 if dropout is None else 2, dropout=0. if dropout is None else dropout)\n",
    "\n",
    "        self.final_dense = final_dense\n",
    "        self.include_meta = include_meta\n",
    "        self.include_embeds = include_embeds\n",
    "        out_layer1_outdim = self.hidden_dim if final_dense else constants['NUM_CLASSES']\n",
    "        if include_meta and include_embeds:\n",
    "            self.out_layer1 = nn.Linear(self.hidden_dim+constants['SF_LEN']+3*constants['WORD_EMBED_DIM'], out_layer1_outdim)\n",
    "        elif include_embeds:\n",
    "            self.out_layer1 = nn.Linear(self.hidden_dim+3*constants['WORD_EMBED_DIM'], out_layer1_outdim)\n",
    "        elif include_meta:\n",
    "            self.out_layer1 = nn.Linear(self.hidden_dim+constants['SF_LEN'], out_layer1_outdim)\n",
    "        else:\n",
    "            self.out_layer1 = nn.Linear(self.hidden_dim, out_layer1_outdim)\n",
    "        if self.final_dense:\n",
    "            self.relu = nn.Tanh()\n",
    "            self.out_layer2 = nn.Linear(self.hidden_dim, constants['NUM_CLASSES'])\n",
    "\n",
    "    def forward(self, text_inputs, user_inputs, subreddit_inputs, metafeats, lengths):\n",
    "        text_inputs = self.embed_module(text_inputs)\n",
    "        user_inputs = self.user_embeds(user_inputs)\n",
    "        subreddit_inputs = self.subreddit_embeds(subreddit_inputs)\n",
    "        if self.prepend_social is True:\n",
    "            inputs = torch.cat([user_inputs, subreddit_inputs, text_inputs], dim=0)\n",
    "        elif self.prepend_social is False:\n",
    "            inputs = torch.cat([text_inputs, user_inputs, subreddit_inputs], dim=0)\n",
    "        else:\n",
    "            inputs = text_inputs\n",
    "            lengths = [l-3 for l in lengths]\n",
    "        inputs  = nn.utils.rnn.pack_padded_sequence(inputs, lengths)\n",
    "        outputs, h = self.rnn(inputs, self.init_hidden)\n",
    "\n",
    "        h, lengths = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        h = h.sum(dim=0).squeeze()\n",
    "        # lengths = torch.FloatTensor(lengths)  # attempt to remove, `lengths` is already torch.Tensor\n",
    "        if constants['CUDA']:\n",
    "            lengths = lengths.cuda()\n",
    "        h = h.t().div(Variable(lengths))\n",
    "        self.h = h\n",
    "#        self.h = h[0][0].t()\n",
    "#        h = h[0][0].t()\n",
    "\n",
    "        final_input = h.t()\n",
    "        if self.include_meta:\n",
    "            final_input = torch.cat([final_input, metafeats.t()], dim=1)\n",
    "        if self.include_embeds:\n",
    "            final_input = torch.cat([final_input, user_inputs.squeeze(), subreddit_inputs[0], subreddit_inputs[1]], dim=1)\n",
    "        if not self.final_dense:\n",
    "            weights = self.out_layer1(final_input)\n",
    "        else:\n",
    "            weights = self.out_layer2(self.relu(self.out_layer1(final_input)))\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6FnHnEUdU15"
   },
   "source": [
    "### Training functions\n",
    "- `load_data(batch_size, max_len)`\n",
    "  - returns batches\n",
    "- `get_embeddings(data)`\n",
    "  - returns tuple `(ids, embeds)`\n",
    "- `train(model, train_data, val_data, test_data, optimizer, epochs=10, log_every=100, log_file=None, save_embeds=False)`\n",
    "  - returns best iteration\n",
    "- `evaluate_auc(model, test_data)`\n",
    "  - returns auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QO9o57FvdUWK"
   },
   "outputs": [],
   "source": [
    "def load_data(batch_size, max_len):\n",
    "    print('Loading train/test data...')\n",
    "    thread_to_sub = {}\n",
    "    with open(constants['POST_INFO']) as fp:\n",
    "        for line in fp:\n",
    "            info = line.split()\n",
    "            source_sub = info[0]\n",
    "            target_sub = info[1]\n",
    "            source_post = info[2].split(\"T\")[0].strip()\n",
    "            target_post = info[6].split(\"T\")[0].strip()\n",
    "            thread_to_sub[source_post] = source_sub\n",
    "            thread_to_sub[target_post] = target_sub\n",
    "\n",
    "    label_map = {}\n",
    "    source_to_dest_sub = {}\n",
    "    with open(constants['LABEL_INFO']) as fp:\n",
    "        for line in fp:\n",
    "            info = line.split(\"\\t\")\n",
    "            source = info[0].split(\",\")[0].split(\"\\'\")[1]\n",
    "            dest = info[0].split(\",\")[1].split(\"\\'\")[1]\n",
    "            label_map[source] = 1 if info[1].strip() == \"burst\" else 0\n",
    "            try:\n",
    "                source_to_dest_sub[source] = thread_to_sub[dest]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    with open(constants['SUBREDDIT_IDS']) as fp:\n",
    "        sub_id_map = {sub:i for i, sub in enumerate(fp.readline().split())}\n",
    "\n",
    "    with open(constants['USER_IDS']) as fp:\n",
    "        user_id_map = {user:i for i, user in enumerate(fp.readline().split())}\n",
    "\n",
    "    with open(constants['PREPROCESSED_DATA']) as fp:\n",
    "        words, users, subreddits, lengths, labels, ids = [], [], [], [], [], []\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.split(\"\\t\")\n",
    "            if info[1] in label_map and info[1] in source_to_dest_sub:\n",
    "                title_words = info[-2].split(\":\")[1].strip().split(\",\")\n",
    "                title_words = title_words[:min(len(title_words), constants['MAX_LEN'])]\n",
    "                if len(title_words) == 0 or title_words[0] == '':\n",
    "                    continue\n",
    "                # words.append(map(int, title_words))\n",
    "                words.append([int(word) for word in title_words])\n",
    "\n",
    "                body_words = info[-1].split(\":\")[1].strip().split(\",\")\n",
    "                body_words = body_words[:min(len(body_words), constants['MAX_LEN']-len(title_words))]\n",
    "                if not (len(body_words) == 0 or body_words[0] == ''):\n",
    "                    # words[-1].extend(map(int, body_words))\n",
    "                    words[-1].extend([int(word) for word in body_words])\n",
    "\n",
    "                words[-1] = [constants['VOCAB_SIZE']+1 if w==-1 else w for w in words[-1]]\n",
    "\n",
    "                if not info[0] in sub_id_map:\n",
    "                    source_sub = constants['NUM_SUBREDDITS']\n",
    "                else:\n",
    "                    source_sub = sub_id_map[info[0]]\n",
    "                dest_sub = source_to_dest_sub[info[1]]\n",
    "                if not dest_sub in sub_id_map:\n",
    "                    dest_sub = constants['NUM_SUBREDDITS']\n",
    "                else:\n",
    "                    dest_sub = sub_id_map[dest_sub]\n",
    "                subreddits.append([source_sub, dest_sub])\n",
    "\n",
    "                users.append([constants['NUM_USERS'] if not info[3] in user_id_map else user_id_map[info[3]]])\n",
    "                ids.append(info[1])\n",
    "\n",
    "                lengths.append(len(words[-1])+3)\n",
    "                labels.append(label_map[info[1]])\n",
    "\n",
    "        batches = []\n",
    "        np.random.seed(0)\n",
    "        for count, i in enumerate(np.random.permutation(len(words))):\n",
    "            if count % batch_size == 0:\n",
    "                batch_words = np.ones((max_len, batch_size), dtype=np.int64) * constants['VOCAB_SIZE']\n",
    "                batch_users = np.ones((1, batch_size), dtype=np.int64) * constants['VOCAB_SIZE']\n",
    "                batch_subs = np.ones((2, batch_size), dtype=np.int64) * constants['VOCAB_SIZE']\n",
    "                batch_lengths = []\n",
    "                batch_labels = []\n",
    "                batch_ids = []\n",
    "            length = min(max_len, len(words[i]))\n",
    "            batch_words[:length, count % batch_size] = words[i][:length]\n",
    "            batch_users[:, count % batch_size] = users[i]\n",
    "            batch_subs[:, count % batch_size] = subreddits[i]\n",
    "            batch_lengths.append(length)\n",
    "            batch_labels.append(labels[i])\n",
    "            batch_ids.append(ids[i])\n",
    "            if count % batch_size == batch_size - 1:\n",
    "                order = np.flip(np.argsort(batch_lengths), axis=0)\n",
    "                batches.append((list(np.array(batch_ids)[order]),\n",
    "                    torch.LongTensor(batch_words[:,order]),\n",
    "                    torch.LongTensor(batch_users[:,order]),\n",
    "                    torch.LongTensor(batch_subs[:,order]),\n",
    "                    list(np.array(batch_lengths)[order]),\n",
    "                    torch.FloatTensor(np.array(batch_labels)[order])))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxUN--Vcdj22"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(data):\n",
    "    embeds = []\n",
    "    ids = []\n",
    "    for batch in data:\n",
    "        id, text, users, subs, lengths, metafeats, labels = batch\n",
    "        text, users, subs, metafeats, labels = Variable(text), Variable(users), Variable(subs), Variable(metafeats), Variable(labels)\n",
    "        model(text, users, subs, metafeats, lengths)\n",
    "        batch_embeds = model.h\n",
    "        embeds.append(batch_embeds.t().data.cpu().numpy())\n",
    "        ids.extend(id)\n",
    "    return ids, np.concatenate(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhUJdiIQeJNS"
   },
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, optimizer,\n",
    "        epochs=10, log_every=100, log_file=None, save_embeds=False):\n",
    "    if not log_file is None:\n",
    "        lg_str = log_file\n",
    "        log_file = open(log_file, \"w\")\n",
    "\n",
    "    ema_loss = None\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_iter = (0., 0,0)\n",
    "    best_test = 0.\n",
    "    embeds = None\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_data)\n",
    "        for i, batch in enumerate(train_data):\n",
    "            _, text, users, subs, lengths, metafeats, labels = batch\n",
    "            text, users, subs, metafeats, labels = Variable(text), Variable(users), Variable(subs), Variable(metafeats), Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text, users, subs, metafeats, lengths)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if ema_loss is None:\n",
    "                # ema_loss = loss.data[0]\n",
    "                ema_loss = loss.item()\n",
    "            else:\n",
    "                # ema_loss = 0.01*loss.data[0] + 0.99*ema_loss\n",
    "                ema_loss = 0.01*loss.item() + 0.99*ema_loss\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(epoch, i, ema_loss)\n",
    "                print(log_file, epoch, i, ema_loss)\n",
    "            if  i % log_every == 0:\n",
    "                auc = evaluate_auc(model, val_data)\n",
    "                print(\"Val AUC\", epoch, i, auc)\n",
    "                if not log_file is None:\n",
    "                    print(\"Val AUC\", epoch, i, auc, file=log_file)\n",
    "                if auc > best_iter[0]:\n",
    "                    best_iter = (auc, epoch, i)\n",
    "                    print(\"New best val!\", best_iter)\n",
    "                    best_test = evaluate_auc(model, test_data)\n",
    "                    if auc > 0.7:\n",
    "                        ids, embeds = get_embeddings(train_data+val_data+test_data)\n",
    "    print('Overall best val: ', best_iter)\n",
    "    if not log_file is None:\n",
    "        print(\"Overall best test:\", best_test, file=log_file)\n",
    "        print(\"Overall best val:\", best_iter, file=log_file)\n",
    "        # print >>log_file, \"Overall best test:\", best_test  # removed in Python 3\n",
    "        # print >>log_file, \"Overall best val:\", best_iter\n",
    "        if not embeds is None and save_embeds:\n",
    "            np.save(open(lg_str+\"-embeds.npy\", \"w\"), embeds)\n",
    "            pickle.dump(ids, open(lg_str+\"-ids.pkl\", \"w\"))\n",
    "    return best_iter[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bl2cXEQGdhFD"
   },
   "outputs": [],
   "source": [
    "def evaluate_auc(model, test_data):\n",
    "    predictions = []\n",
    "    gold_labels = []\n",
    "    for batch in test_data:\n",
    "        _, text, users, subs, lengths, metafeats, labels = batch\n",
    "        if constants['CUDA']:\n",
    "            gold_labels.extend(labels.cpu().numpy().tolist())\n",
    "        else:\n",
    "            gold_labels.extend(labels.numpy().tolist())\n",
    "        text, users, subs, metafeats, labels = Variable(text), Variable(users), Variable(subs), Variable(metafeats), Variable(labels)\n",
    "        outputs = model(text, users, subs, metafeats, lengths)\n",
    "        if constants['CUDA']:\n",
    "            predictions.extend(outputs.data.squeeze().cpu().numpy().tolist())\n",
    "        else:\n",
    "            predictions.extend(outputs.data.squeeze().numpy().tolist())\n",
    "\n",
    "    auc = roc_auc_score(gold_labels, predictions)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03XtVyfbfJHG"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhSAKGkXfdWf"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'learning_rate': 0.01,\n",
    "    'hidden_dim': 64,\n",
    "    'log_file': None,  # Where to log the model training details (string)\n",
    "    'save_embeds': False,  # Whether to save the hidden-state LSTM embeddings that are generated, stored based on the log_file name used above\n",
    "    'dropout': 0.2,  # Dropout rate for inter-LSTM layers in 2-layer LSTM.\n",
    "    'single_layer': False,  # Use single-layer LSTM (implies that dropout param is ignored)\n",
    "\n",
    "    # 11/3 -- note that this isn't working if True from matmul dim errors...default was false in\n",
    "    # original though so leaving it this way.\n",
    "    'include_meta': False,  # Include metadata/hand-crafted features in final layer of model.\n",
    "\n",
    "    'final_dense': False,  # Include an extra Linear+ReLU layer before the softmax.\n",
    "    'lstm_append_social': False,  # Append the social embeddings instead of prepending them to LSTM input.\n",
    "    'lstm_no_social': False,  # Do not include social embeddings in LSTM input.\n",
    "    'final_layer_social': False,  # (Also) include social embeddings in the final layer.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pmWQmyRhP_2"
   },
   "outputs": [],
   "source": [
    "dropout = None if args['single_layer'] else args['dropout']\n",
    "\n",
    "if args['lstm_append_social'] and args['lstm_no_social']:\n",
    "    raise Exception(\"Only one of --lstm_append_social and --lstm_no_social can be True at a time.\")\n",
    "\n",
    "if args['log_file'] is None and args['save_embeds']:\n",
    "    raise Exception(\"A log file must be specified if you want to store the LSTM embeddings of the posts.\")\n",
    "\n",
    "if args['lstm_append_social'] or args['lstm_no_social']:\n",
    "    prepend_social = None if args['lstm_no_social'] else False\n",
    "else:\n",
    "    prepend_social = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljArdIom9pQf"
   },
   "outputs": [],
   "source": [
    "print(\"Loading training data\")\n",
    "# WE HAVE PRE-CONSTRUCTED TRAIN/VAL/TEST DATA USING load_data\n",
    "# this avoids re-doing all the pre-processing everytime the code is\n",
    "# run. This data is fixed to a batch size of 512.\n",
    "train_data = pickle.load(open(constants['TRAIN_DATA'], 'rb'))\n",
    "val_data = pickle.load(open(constants['VAL_DATA'], 'rb'))\n",
    "test_data = pickle.load(open(constants['TEST_DATA'], 'rb'))\n",
    "\n",
    "print(len(train_data)*constants['BATCH_SIZE'], \"training examples\", len(val_data)*512, \"validation examples\")\n",
    "print(sum([i for batch in train_data for i in batch[-1]]), \"positive training\", sum([i for batch in val_data for i in batch[-1]]), \"positive validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Crgiq8IKeL-R"
   },
   "outputs": [],
   "source": [
    "# annoying checks for CUDA switches....\n",
    "if constants['CUDA']:\n",
    "    for i in range(len(train_data)):\n",
    "        batch = train_data[i]\n",
    "        metafeats = batch[5]\n",
    "        train_data[i] = (batch[0],\n",
    "                batch[1].cuda(),\n",
    "                batch[2].cuda(),\n",
    "                batch[3].cuda(),\n",
    "                batch[4],\n",
    "                metafeats.cuda(),\n",
    "                batch[6].cuda())\n",
    "\n",
    "    for i in range(len(val_data)):\n",
    "        batch = val_data[i]\n",
    "        metafeats = batch[5]\n",
    "        val_data[i] = (batch[0],\n",
    "                batch[1].cuda(),\n",
    "                batch[2].cuda(),\n",
    "                batch[3].cuda(),\n",
    "                batch[4],\n",
    "                metafeats.cuda(),\n",
    "                batch[6].cuda())\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        batch = test_data[i]\n",
    "        metafeats = batch[5]\n",
    "        test_data[i] = (batch[0],\n",
    "                batch[1].cuda(),\n",
    "                batch[2].cuda(),\n",
    "                batch[3].cuda(),\n",
    "                batch[4],\n",
    "                metafeats.cuda(),\n",
    "                batch[6].cuda())\n",
    "\n",
    "best_auc = (0,\"\")\n",
    "model = SocialLSTM(args['hidden_dim'], prepend_social=prepend_social, dropout=args['dropout'], include_embeds=args['final_layer_social'],\n",
    "        include_meta=args['include_meta'], final_dense=args['final_dense'])\n",
    "if constants['CUDA']:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr=args['learning_rate'])\n",
    "auc = train(model, train_data, val_data, test_data, optimizer, epochs=10, log_file=args['log_file'], save_embeds=args['save_embeds'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs224w-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
